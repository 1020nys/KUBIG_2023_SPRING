{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["PF93Fx7YQu-U","sF0VWa6SRmSA"],"machine_shape":"hm","authorship_tag":"ABX9TyNNIFbMKLZvlifQ8m/21Lr1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 패키지 설치"],"metadata":{"id":"PF93Fx7YQu-U"}},{"cell_type":"code","source":["# Optuna 설치\n","!pip install --quiet --no-cache-dir git+https://github.com/optuna/optuna\n","\n","# XGB GPU 버전 설치\n","!pip uninstall --quiet -y xgboost\n","!pip install --quiet xgboost\n","\n","# LGBM GPU 버전 설치\n","! git clone --recursive https://github.com/Microsoft/LightGBM\n","! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpWhpoEBQ4pS","executionInfo":{"status":"ok","timestamp":1677659123838,"user_tz":-540,"elapsed":162914,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"97ed7b2a-f0fc-43eb-d09b-86425cbde87b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for UNKNOWN (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCloning into 'LightGBM'...\n","remote: Enumerating objects: 29207, done.\u001b[K\n","remote: Counting objects: 100% (3082/3082), done.\u001b[K\n","remote: Compressing objects: 100% (277/277), done.\u001b[K\n","remote: Total 29207 (delta 2942), reused 2841 (delta 2805), pack-reused 26125\u001b[K\n","Receiving objects: 100% (29207/29207), 20.38 MiB | 27.83 MiB/s, done.\n","Resolving deltas: 100% (21708/21708), done.\n","Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n","Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n","Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n","Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n","Cloning into '/content/LightGBM/external_libs/compute'...\n","remote: Enumerating objects: 21733, done.        \n","remote: Counting objects: 100% (5/5), done.        \n","remote: Compressing objects: 100% (4/4), done.        \n","remote: Total 21733 (delta 1), reused 3 (delta 1), pack-reused 21728        \n","Receiving objects: 100% (21733/21733), 8.51 MiB | 33.52 MiB/s, done.\n","Resolving deltas: 100% (17567/17567), done.\n","Cloning into '/content/LightGBM/external_libs/eigen'...\n","remote: Enumerating objects: 118512, done.        \n","remote: Counting objects: 100% (194/194), done.        \n","remote: Compressing objects: 100% (113/113), done.        \n","remote: Total 118512 (delta 102), reused 146 (delta 81), pack-reused 118318        \n","Receiving objects: 100% (118512/118512), 102.97 MiB | 36.28 MiB/s, done.\n","Resolving deltas: 100% (97929/97929), done.\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser'...\n","remote: Enumerating objects: 781, done.        \n","remote: Counting objects: 100% (180/180), done.        \n","remote: Compressing objects: 100% (66/66), done.        \n","remote: Total 781 (delta 124), reused 131 (delta 103), pack-reused 601        \n","Receiving objects: 100% (781/781), 833.45 KiB | 931.00 KiB/s, done.\n","Resolving deltas: 100% (395/395), done.\n","Cloning into '/content/LightGBM/external_libs/fmt'...\n","remote: Enumerating objects: 31606, done.        \n","remote: Counting objects: 100% (127/127), done.        \n","remote: Compressing objects: 100% (62/62), done.        \n","remote: Total 31606 (delta 61), reused 95 (delta 51), pack-reused 31479        \n","Receiving objects: 100% (31606/31606), 13.85 MiB | 32.02 MiB/s, done.\n","Resolving deltas: 100% (21387/21387), done.\n","Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n","Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n","Submodule path 'external_libs/fast_double_parser': checked out 'ace60646c02dc54c57f19d644e49a61e7e7758ec'\n","Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n","Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n","remote: Enumerating objects: 20749, done.        \n","remote: Counting objects: 100% (3791/3791), done.        \n","remote: Compressing objects: 100% (846/846), done.        \n","remote: Total 20749 (delta 3021), reused 3003 (delta 2945), pack-reused 16958        \n","Receiving objects: 100% (20749/20749), 12.30 MiB | 28.43 MiB/s, done.\n","Resolving deltas: 100% (16294/16294), done.\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n","remote: Enumerating objects: 1358, done.        \n","remote: Counting objects: 100% (202/202), done.        \n","remote: Compressing objects: 100% (110/110), done.        \n","remote: Total 1358 (delta 110), reused 157 (delta 84), pack-reused 1156        \n","Receiving objects: 100% (1358/1358), 7.15 MiB | 25.33 MiB/s, done.\n","Resolving deltas: 100% (882/882), done.\n","Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n","Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n","Submodule path 'external_libs/fmt': checked out 'b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9'\n","-- The C compiler identification is GNU 9.4.0\n","-- The CXX compiler identification is GNU 9.4.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n","-- Found OpenMP: TRUE (found version \"4.5\")  \n","-- Looking for CL_VERSION_2_2\n","-- Looking for CL_VERSION_2_2 - found\n","-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n","-- OpenCL include directory: /usr/include\n","-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.56.0\") found components: filesystem system \n","-- Performing Test MM_PREFETCH\n","-- Performing Test MM_PREFETCH - Success\n","-- Using _mm_prefetch\n","-- Performing Test MM_MALLOC\n","-- Performing Test MM_MALLOC - Success\n","-- Using _mm_malloc\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/LightGBM/build\n","[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_capi_objs.dir/src/c_api.cpp.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/boosting.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/cuda/cuda_score_updater.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/sample_strategy.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/cuda/cuda_utils.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/bin.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config_auto.cpp.o\u001b[0m\n","[ 22%] Built target lightgbm_capi_objs\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_column_data.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_metadata.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_row_data.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_tree.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset_loader.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/file_io.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/json11.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/metadata.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/parser.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/train_share_states.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/tree.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_binary_metric.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_pointwise_metric.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_regression_metric.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/metric.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linker_topo.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_socket.cpp.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/network.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_binary_objective.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_multiclass_objective.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_rank_objective.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_regression_objective.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/objective_function.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_best_split_finder.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_data_partition.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_histogram_constructor.cpp.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_leaf_splits.cpp.o\u001b[0m\n","[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_single_gpu_tree_learner.cpp.o\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n","In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/data_parallel_tree_learner.cpp:9\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n","   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n","      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/feature_parallel_tree_learner.cpp:8\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n","   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n","      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.cpp:7\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n","   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n","      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n","In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/tree_learner.cpp:7\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n","   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n","      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n","In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/content/LightGBM/src/treelearner/voting_parallel_tree_learner.cpp:11\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n","   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n","      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","[ 92%] Built target lightgbm_objs\n","[ 96%] \u001b[32m\u001b[1mLinking CXX shared library ../lib_lightgbm.so\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n","[ 98%] Built target _lightgbm\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../lightgbm\u001b[0m\n","[100%] Built target lightgbm\n","running install\n","running build\n","running build_py\n","Generating grammar tables from /usr/lib/python3.8/lib2to3/Grammar.txt\n","Generating grammar tables from /usr/lib/python3.8/lib2to3/PatternGrammar.txt\n","creating build\n","creating build/lib\n","creating build/lib/lightgbm\n","copying lightgbm/compat.py -> build/lib/lightgbm\n","copying lightgbm/plotting.py -> build/lib/lightgbm\n","copying lightgbm/libpath.py -> build/lib/lightgbm\n","copying lightgbm/basic.py -> build/lib/lightgbm\n","copying lightgbm/callback.py -> build/lib/lightgbm\n","copying lightgbm/dask.py -> build/lib/lightgbm\n","copying lightgbm/sklearn.py -> build/lib/lightgbm\n","copying lightgbm/engine.py -> build/lib/lightgbm\n","copying lightgbm/__init__.py -> build/lib/lightgbm\n","running egg_info\n","creating lightgbm.egg-info\n","writing lightgbm.egg-info/PKG-INFO\n","writing dependency_links to lightgbm.egg-info/dependency_links.txt\n","writing requirements to lightgbm.egg-info/requires.txt\n","writing top-level names to lightgbm.egg-info/top_level.txt\n","writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n","reading manifest file 'lightgbm.egg-info/SOURCES.txt'\n","reading manifest template 'MANIFEST.in'\n","no previously-included directories found matching 'build'\n","warning: no files found matching 'LICENSE'\n","warning: no files found matching '*.txt'\n","warning: no files found matching '*.so' under directory 'lightgbm'\n","warning: no files found matching 'compile/CMakeLists.txt'\n","warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n","warning: no files found matching '*.so' under directory 'compile'\n","warning: no files found matching '*.dll' under directory 'compile/Release'\n","warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n","warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n","warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n","warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n","warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n","warning: no files found matching '*' under directory 'compile/include'\n","warning: no files found matching '*' under directory 'compile/src'\n","warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n","warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n","warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n","warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n","warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n","writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n","copying lightgbm/VERSION.txt -> build/lib/lightgbm\n","copying lightgbm/py.typed -> build/lib/lightgbm\n","running install_lib\n","copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/dask.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","copying build/lib/lightgbm/py.typed -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","Installing lib_lightgbm from: ['/content/LightGBM/lib_lightgbm.so']\n","copying /content/LightGBM/lib_lightgbm.so -> /usr/local/lib/python3.8/dist-packages/lightgbm\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/compat.py to compat.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/plotting.py to plotting.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/libpath.py to libpath.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/basic.py to basic.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/callback.py to callback.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/dask.py to dask.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py to sklearn.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/engine.py to engine.cpython-38.pyc\n","byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/__init__.py to __init__.cpython-38.pyc\n","running install_egg_info\n","Copying lightgbm.egg-info to /usr/local/lib/python3.8/dist-packages/lightgbm-3.3.5.99-py3.8.egg-info\n","running install_scripts\n"]}]},{"cell_type":"code","source":["!pip install haversine"],"metadata":{"id":"CfByAFEQRZQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677659125959,"user_tz":-540,"elapsed":2127,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"2d96571b-bb91-47f3-e743-130d5b29d5cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting haversine\n","  Downloading haversine-2.8.0-py2.py3-none-any.whl (7.7 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.8.0\n"]}]},{"cell_type":"markdown","source":["# Data 불러오기"],"metadata":{"id":"sF0VWa6SRmSA"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from haversine import haversine\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.decomposition import PCA\n","import gc\n","from google.colab import drive\n","drive.mount('/content/drive')\n","PATH = \"/content/drive/MyDrive/KUBIG/콘테스트\"\n","\n","\n","def csv_to_parquet(csv_path, save_name):\n","    df = pd.read_csv(csv_path)\n","    df.to_parquet(f'./{save_name}.parquet')\n","    del df\n","    gc.collect()\n","    print(save_name, 'Done.')\n","\n","\n","csv_to_parquet(PATH+'/train.csv', 'train')\n","csv_to_parquet(PATH+'/test.csv', 'test')\n","\n","\n","train = pd.read_parquet('./train.parquet')\n","test = pd.read_parquet('./test.parquet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ya7XI1M9RHAp","executionInfo":{"status":"ok","timestamp":1677659182980,"user_tz":-540,"elapsed":57024,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"4c57673a-b285-4f07-dbb7-f293e582b105"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","train Done.\n","test Done.\n"]}]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"3qdRq-NCjPlA"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"FBOHf9PCFgWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_parquet('./train.parquet')\n","test = pd.read_parquet('./test.parquet')\n","\n","######## 날짜 ########\n","## id, height_restricted, vehicle_restricted은 전 데이터에 걸쳐 0이므로 drop\n","train.drop(columns=['id', 'height_restricted', 'vehicle_restricted'], inplace=True)\n","test.drop(columns=['id', 'height_restricted', 'vehicle_restricted'], inplace=True)\n","\n","\n","## base_date 전처리- date type으로 처리\n","train['date'] = pd.to_datetime(train['base_date'], format='%Y%m%d')\n","test['date'] = pd.to_datetime(train['base_date'], format='%Y%m%d')\n","\n","\n","## date 전처리- 년, 월, 일 컬럼 추가\n","train['year'] = train['date'].dt.year\n","train['month'] = train['date'].dt.month\n","train['day'] = train['date'].dt.day\n","\n","test['year'] = test['date'].dt.year\n","test['month'] = test['date'].dt.month\n","test['day'] = test['date'].dt.day\n","\n","\n","## date 전처리- 년+월만 있는 컬럼 추가 (입도 현황)\n","train['year_month'] = train['base_date'].astype(str).str[:6]\n","test['year_month'] = test['base_date'].astype(str).str[:6]\n","\n","\n","## date 전처리- 계절 컬럼 추가 (반복문보다 함수화가 더 빨라서 이렇게 진행)\n","def seasons(i) :\n","    if i in [3, 4, 5] :\n","        return 'spring'\n","    elif i in [6, 7, 8] :\n","        return 'summer'\n","    elif i in [9, 10, 11] :\n","        return 'fall'\n","    else :\n","        return 'winter'\n","\n","train['season'] = train['month'].apply(seasons)\n","test['season'] = test['month'].apply(seasons)\n","\n","\n","\n","## 2021, 2022 공휴일 +- 1일에 대한 리스트\n","holidays = ['20210101', '20210102', \n","            '20210210', '20210211', '20210212', '20210213', '20210214'\n","            '20210228', '20210301', '20210302',\n","            '20210504', '20210505', '20210506',\n","            '20210518', '20210519', '20210520',\n","            '20210607', '20210606', '20210607',\n","            '20210814', '20210815', '20210816', '20210817'\n","            '20210919', '20210920', '20210921', '20210922', '20210923'\n","            '20211002', '20211003', '20211004', '20211005',\n","            '20211008', '20211009', '20211010', '20211011', '20211012',\n","            '20211226', '20211225', '20211227'\n","\n","            '20211231', '20220101', '20220102',\n","            '20220130', '20220131', '20220201', '20220202', '20220203',\n","            '20220228', '20220301', '20220302',\n","            '20220308', '20220309', '20220310',  \n","            '20220504', '20220505', '20220506',\n","            '20220507', '20220508', '20220509',\n","            '20220531', '20220601', '20220602',\n","            '20220814', '20220815', '20220816',\n","            '20220908', '20220909', '20220910', '20220911', '20220912', '20220913',\n","            '20221002', '20221003', '20221004',\n","            '20221008', '20221009', '20221010', '20221011',\n","            '20221224', '20221225', '20221226']\n","\n","train['holiday'] = train['base_date'].astype(str).isin(holidays)\n","test['holiday'] = test['base_date'].astype(str).isin(holidays)\n","\n","\n","## 휴가철\n","vacation = [12, 1, 2, 6, 7, 8]\n","\n","def vacations(i) :\n","    if i in vacation :\n","        return 'yes'\n","    else :\n","        return 'no'\n","\n","train['vacation'] = train['month'].apply(vacations)\n","test['vacation'] = test['month'].apply(vacations)\n","\n","\n","\n","\n","\n","\n","\n","######## 요일 ########\n","## 요일 -> 숫자 (일요일 시작)\n","days = {'일':1, '월':2, '화':3, '수':4, '목':5, '금':6, '토':7}\n","\n","train['day_of_week'] = train['day_of_week'].replace(days)\n","test['day_of_week'] = test['day_of_week'].replace(days)\n","\n","\n","## 주말인가?\n","def weekends(i) :\n","    if i in [1, 7] :\n","        return 'yes'\n","    else :\n","        return 'no'\n","\n","train['weekend'] = train['day_of_week'].apply(weekends)\n","test['weekend'] = test['day_of_week'].apply(weekends)\n","\n","\n","\n","\n","\n","\n","######## 시간대 ########\n","## 오전, 오후, 저녁, 새벽으로 나눔 (6시간 단위)\n","def times(i) :\n","    if i in [6, 7, 8, 9, 10, 11] :\n","        return 'morning'\n","    elif i in [12, 13, 14, 15, 16, 17] :\n","        return 'noon'\n","    elif i in [18, 19, 20, 21, 22, 23] :\n","        return 'night'\n","    else :\n","        return 'dawn'\n","\n","train['time'] = train['base_hour'].apply(times)\n","test['time'] = test['base_hour'].apply(times)\n","\n","\n","## 등하교, 출퇴근, 주말은 전부 rush_hour이 아닌거로\n","def rush_hours(i) :\n","    if i in [7, 8, 9, 17, 18, 19] :\n","        return 'yes'\n","    else :\n","        return 'no'\n","\n","train['rush_hour'] = train['base_hour'].loc[train['weekend']=='no'].apply(rush_hours)\n","train['rush_hour'].fillna('no', inplace=True)\n","test['rush_hour'] = test['base_hour'].loc[train['weekend']=='no'].apply(rush_hours)\n","test['rush_hour'].fillna('no', inplace=True)\n","\n","\n","\n","\n","\n","\n","\n","\n","######## 도로 관련 데이터 ########\n","## multi_linked, connect_code, start_node_name, end_node_name drop\n","train.drop(columns=['multi_linked', 'connect_code', 'start_node_name', 'end_node_name'], inplace=True)\n","test.drop(columns=['multi_linked', 'connect_code', 'start_node_name', 'end_node_name'], inplace=True)\n","\n","\n","## maximum_speed_limit : 최고 제한 속도보다 target이 빠른 경우 제거 (만약 성능 떨어지면 빼고 해도됨)\n","#train[(train['maximum_speed_limit']<train['target'])].head()\n","#print(len(train[(train['maximum_speed_limit']<train['target'])]) / len(train)) # 대략 9.7%의 데이터 drop 예정\n","#train = train.loc[train['maximum_speed_limit']>=train['target']]\n","\n","\n","## weight_restricted 범주화\n","print(train['weight_restricted'].unique())\n","print(test['weight_restricted'].unique())\n","\n","\n","## 1. 무게별로 범주 다르게\n","weights = {0:0, 32400:1, 43200:2, 50000:3}\n","train['weight_restricted'] = train['weight_restricted'].astype(int).replace(weights)\n","test['weight_restricted'] = test['weight_restricted'].astype(int).replace(weights)\n","\n","\n","## 2. target이 비슷한것끼리 범주화 (1을 쓰지 않는 경우 이렇게도 활용 가능)\n","# weights = {0:0, 32300:1, 50000:1, 43200:2}\n","# train['weight_restricted'] = train['weight_restricted'].replace(weights)\n","# test['weight_restricted'] = test['weight_restricted'].replace(weights)\n","\n","\n","## 3. road_type과 road_rating 묶어서 범주로 (만약 따로따로 넣는게 낫다면 해당 코드 안써도 됨)\n","train['road_type_rating'] = train['road_rating'].astype(str)+'_'+train['road_type'].astype(str)\n","test['road_type_rating'] = test['road_rating'].astype(str)+'_'+test['road_type'].astype(str)\n","\n","\n","## road_type, road_rating drop (이건 필요하다고 생각하면 각주 해제 후 사용할 것)\n","# train.drop(columns=['road_rating', 'road_type'], inplace=True)\n","# test.drop(columns=['road_rating', 'road_type'], inplace=True)\n","\n","\n","## start_turn_restricted와 end_turn_restricted encoding\n","yes_no = {'없음':0, '있음':1}\n","train['start_turn_restricted'] = train['start_turn_restricted'].replace(yes_no)\n","test['start_turn_restricted'] = test['start_turn_restricted'].replace(yes_no)\n","train['end_turn_restricted'] = train['end_turn_restricted'].replace(yes_no)\n","test['end_turn_restricted'] = test['end_turn_restricted'].replace(yes_no)\n","\n","\n","## 이를 합친 turn_restricted\n","train['turn_restricted'] = train['start_turn_restricted'] + train['end_turn_restricted']\n","test['turn_restricted'] = test['start_turn_restricted'] + train['end_turn_restricted']\n","\n","\n","## start_turn_restricted와 end_turn_restricted drop (필요시 주석 해제 후 사용)\n","# train.drop(columns=['start_turn_restricted', 'end_turn_restricted'], inplace=True)\n","# test.drop(columns=['start_turn_restricted', 'end_turn_restricted'], inplace=True)\n","\n","\n","## 2. 거리 계산 함수\n","def distance(x) :\n","    start_location = tuple(zip(x['start_latitude'], x['start_longitude']))\n","    end_location = tuple(zip(x['end_latitude'], x['end_longitude']))\n","    hsine = [haversine(s, e, unit='km') for s, e in zip(start_location, end_location)]\n","    return hsine\n","train['road_distance'] = distance(train)\n","test['road_distance'] = distance(test)\n","\n","\n","## 3. 구역 Clustering\n","def make_cluster(x) : \n","    x_lat_long = x[['start_latitude', 'start_longitude']]\n","\n","    k_mean = KMeans(n_clusters=6, max_iter=1000, random_state = 31)\n","    location_cluster = k_mean.fit_predict(x_lat_long)\n","    return location_cluster\n","\n","train['location_cluster'] = make_cluster(train)\n","test['location_cluster'] = make_cluster(test)\n","\n","\n","## 도로명에 - 라는 결측치가 존재함으로 확인되므로 확실한 시작-끝 지점 위-경도를 사용\n","train['road_lat_long'] = train['start_latitude'].astype(str)+'_'+train['start_longitude'].astype(str)+'_'+train['end_latitude'].astype(str)+'_'+train['end_longitude'].astype(str)\n","test['road_lat_long'] = test['start_latitude'].astype(str)+'_'+test['start_longitude'].astype(str)+'_'+test['end_latitude'].astype(str)+'_'+test['end_longitude'].astype(str)\n","\n","\n","\n","# Label encoding\n","\n","str_col = ['road_name', 'year_month', 'season', 'vacation', 'weekend', 'time', 'rush_hour', 'road_type_rating', 'road_lat_long', 'holiday', 'year_month']\n","for i in str_col:\n","    le = LabelEncoder()\n","    le=le.fit(train[i])\n","    train[i]=le.transform(train[i])\n","    \n","    for label in np.unique(test[i]):\n","        if label not in le.classes_: \n","            le.classes_ = np.append(le.classes_, label)\n","    test[i]=le.transform(test[i])\n","\n","\n","\n","## 위-경도로된 도로별 시간별 / 요일별 속도 평균 평균 계산\n","## 도로별 시간별 속도 평균\n","train['road_hour'] = train['road_lat_long'].astype(str)+'_'+train['base_hour'].astype(str)\n","test['road_hour'] = test['road_lat_long'].astype(str)+'_'+test['base_hour'].astype(str)\n","hour_mean = train.groupby(['road_hour'])[['target']].agg('mean').reset_index()\n","hour_mean.columns = ['road_hour', 'road_hour_mean']\n","train = train.merge(hour_mean, how='left', on='road_hour')\n","test = test.merge(hour_mean, how='left', on='road_hour')\n","\n","\n","## 도로별 요일별 속도 평균\n","train['road_day'] = train['road_lat_long'].astype(str)+'_'+train['day_of_week'].astype(str)\n","test['road_day'] = test['road_lat_long'].astype(str)+'_'+test['day_of_week'].astype(str)\n","hour_mean = train.groupby(['road_day'])[['target']].agg('mean').reset_index()\n","hour_mean.columns = ['road_day', 'road_day_mean']\n","train = train.merge(hour_mean, how='left', on='road_day')\n","test = test.merge(hour_mean, how='left', on='road_day')\n","\n","\n","## 이상치 제거 함수 - 이상치를 제거해야할 column을 못찾겠어서 사용하지 않았으나 필요시 사용해볼것\n","# def outlier_detect(cols, df) :\n","#     for col in cols :\n","#         Q1 = df[col].quantile(0.25)\n","#         Q3 = df[col].quantile(0.75)\n","#         iqr = Q3 - Q1\n","#         df = df[(df[col] <= 1.5 * Q3) & (df[col] >= 1.5 * Q1)]\n","#         df = df.reset_index(drop=True)\n","#     return df\n","\n","\n","## 이상치를 제거해야하는 columns\n","# remove_outliers_cols = []\n","\n","\n","\n","\n","\n","\n","\n","\n","######## 외부 데이터 ########\n","## 입도 관관갱 컬럼 (inflow)\n","#inflow_train = pd.DataFrame({'year_month':['202109', '202110', '202111', '202112', '202201', '202202',\n","#       '202203', '202205', '202206', '202207'],\n","#       'inflow':[872396, 1222094, 1204344, 1090607, 1170802, 1029503, 873086, 1306537, 1283470, 1263332]})\n","#inflow_test = pd.DataFrame({'year_month' : '202208', 'inflow': [1281608]})\n","\n","#train = train.merge(inflow_train, how='left', on='year_month')\n","#test = test.merge(inflow_test, how='left', on='year_month')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hx5SdeqNSP1J","executionInfo":{"status":"ok","timestamp":1677664336107,"user_tz":-540,"elapsed":139389,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"cff0db09-ef95-41b3-bec2-51d21448cab2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[32400.     0. 43200. 50000.]\n","[    0. 43200. 32400. 50000.]\n"]}]},{"cell_type":"code","source":["train = train[['base_date', 'day_of_week', 'base_hour', 'lane_count', 'road_rating',\n","       'road_name', 'maximum_speed_limit', 'weight_restricted', 'road_type',\n","       'start_latitude', 'start_longitude', 'start_turn_restricted',\n","       'end_latitude', 'end_longitude', 'end_turn_restricted', 'date', \n","       'year', 'month', 'day', 'year_month', 'season', 'holiday',\n","       'vacation', 'weekend', 'time', 'rush_hour', 'road_type_rating',\n","       'turn_restricted', 'road_distance', 'location_cluster', 'road_lat_long',\n","       'road_hour', 'road_hour_mean', 'road_day', 'road_day_mean', 'target']]\n","\n","test = test[['base_date', 'day_of_week', 'base_hour', 'lane_count', 'road_rating',\n","       'road_name', 'maximum_speed_limit', 'weight_restricted', 'road_type',\n","       'start_latitude', 'start_longitude', 'start_turn_restricted',\n","       'end_latitude', 'end_longitude', 'end_turn_restricted', 'date', \n","       'year', 'month', 'day', 'year_month', 'season', 'holiday',\n","       'vacation', 'weekend', 'time', 'rush_hour', 'road_type_rating',\n","       'turn_restricted', 'road_distance', 'location_cluster', 'road_lat_long',\n","       'road_hour', 'road_hour_mean', 'road_day', 'road_day_mean']]"],"metadata":{"id":"MHeqch_CkWod"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LGBM2"],"metadata":{"id":"KYcmZehKv4KX"}},{"cell_type":"code","source":["from lightgbm import LGBMRegressor\n","\n","skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","lgbm_param1 = {\n","    'objective' : 'regression',\n","    'device' : 'gpu',\n","    'metric' : 'mae'\n","}\n","\n","\n","lgbm_pred1 = np.zeros(target.shape[0])\n","i = 0\n","lgbm_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    lgbm = LGBMRegressor(**lgbm_param1)\n","    lgbm.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], eval_metric = 'mae')\n","\n","    val_pred = lgbm.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    lgbm_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = lgbm.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LaoWQgsv5iY","executionInfo":{"status":"ok","timestamp":1677667307334,"user_tz":-540,"elapsed":231286,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"3d4ed352-a603-45c6-dda8-24eff755386f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2143\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.140096 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788469\n","1 Fold MAE = 3.415904807688217\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2142\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.099029 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788468\n","2 Fold MAE = 3.417923432640889\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2142\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.104438 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788436\n","3 Fold MAE = 3.4141924862057085\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2140\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.152997 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788426\n","4 Fold MAE = 3.4142733162881123\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2141\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.093139 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788385\n","5 Fold MAE = 3.4171151318168476\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2143\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.095063 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788412\n","6 Fold MAE = 3.415092252649312\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2139\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.151638 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788430\n","7 Fold MAE = 3.416191967191495\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2140\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.095007 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788456\n","8 Fold MAE = 3.4130511081189736\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2142\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.088966 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788478\n","9 Fold MAE = 3.4270538861271884\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2140\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (96.84 MB) transferred to GPU in 0.093153 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788460\n","10 Fold MAE = 3.404317186426473\n","\n","AVG of MAE = 3.1270175287352213\n"]}]},{"cell_type":"code","source":["import optuna\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def objective(trial,data=X,target=y):\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2 ,random_state=2023)\n","    param = {\n","        'device' : 'gpu',\n","        'objective': 'regression',\n","        'verbose': -1,\n","        'metric': 'mae', \n","        'num_leaves': trial.suggest_int('num_leaves', 2, 1024, step=1, log=True), \n","        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n","        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n","        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n","        'max_depth': trial.suggest_int('max_depth',3, 15),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n","        'subsample': trial.suggest_loguniform('subsample', 0.4, 1)\n","    }\n","\n","    # Generate model\n","    model = LGBMRegressor(**param)\n","    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)], eval_metric = 'mae')\n","    \n","    preds = model.predict(X_valid)\n","    \n","    mae = mean_absolute_error(y_valid, preds)\n","    \n","    return mae"],"metadata":{"id":"8d0gwyRGxCfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from optuna.samplers import TPESampler\n","\n","sampler = TPESampler(seed=2023)\n","study = optuna.create_study(\n","    study_name=\"lgbm_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=10)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxrbPNBxyQHI","executionInfo":{"status":"ok","timestamp":1677669408054,"user_tz":-540,"elapsed":1713782,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"410a6623-217c-4df8-8afc-50032d7b09af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-03-01 10:48:13,749]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n","\u001b[32m[I 2023-03-01 10:51:00,994]\u001b[0m Trial 0 finished with value: 3.2443368268137784 and parameters: {'num_leaves': 12, 'colsample_bytree': 0.9671267355368443, 'reg_alpha': 0.5880522554504738, 'reg_lambda': 1.2659609350429124, 'max_depth': 4, 'n_estimators': 1457, 'min_child_samples': 7, 'subsample': 0.7788820527269581}. Best is trial 0 with value: 3.2443368268137784.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:52:16,072]\u001b[0m Trial 1 finished with value: 3.1789818199687114 and parameters: {'num_leaves': 46, 'colsample_bytree': 0.8634805716431471, 'reg_alpha': 0.456373260425773, 'reg_lambda': 5.013822646640289, 'max_depth': 8, 'n_estimators': 538, 'min_child_samples': 39, 'subsample': 0.4640416013784816}. Best is trial 1 with value: 3.1789818199687114.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:53:26,775]\u001b[0m Trial 2 finished with value: 3.2735497591313423 and parameters: {'num_leaves': 14, 'colsample_bytree': 0.7540969842279664, 'reg_alpha': 0.39099140113992126, 'reg_lambda': 0.35648210404088165, 'max_depth': 10, 'n_estimators': 690, 'min_child_samples': 35, 'subsample': 0.5648196356324413}. Best is trial 1 with value: 3.1789818199687114.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:57:11,506]\u001b[0m Trial 3 finished with value: 3.307796778013891 and parameters: {'num_leaves': 5, 'colsample_bytree': 0.7311855509058784, 'reg_alpha': 0.45492722454725665, 'reg_lambda': 1.9586383749144975, 'max_depth': 7, 'n_estimators': 2799, 'min_child_samples': 77, 'subsample': 0.8105464700196405}. Best is trial 1 with value: 3.1789818199687114.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:58:12,700]\u001b[0m Trial 4 finished with value: 3.116831427448392 and parameters: {'num_leaves': 74, 'colsample_bytree': 0.9374863441651909, 'reg_alpha': 0.8103383036559854, 'reg_lambda': 9.80557225759712, 'max_depth': 14, 'n_estimators': 418, 'min_child_samples': 83, 'subsample': 0.5302387557044125}. Best is trial 4 with value: 3.116831427448392.\u001b[0m\n","\u001b[32m[I 2023-03-01 11:02:31,032]\u001b[0m Trial 5 finished with value: 3.2398659867171538 and parameters: {'num_leaves': 8, 'colsample_bytree': 0.8217170632657635, 'reg_alpha': 0.5534203817538123, 'reg_lambda': 6.255264410706003, 'max_depth': 4, 'n_estimators': 2920, 'min_child_samples': 44, 'subsample': 0.7748883476056193}. Best is trial 4 with value: 3.116831427448392.\u001b[0m\n","\u001b[32m[I 2023-03-01 11:05:21,453]\u001b[0m Trial 6 finished with value: 2.9920208906812245 and parameters: {'num_leaves': 114, 'colsample_bytree': 0.7654675794588895, 'reg_alpha': 0.1871725370949876, 'reg_lambda': 7.297792387589718, 'max_depth': 14, 'n_estimators': 1236, 'min_child_samples': 15, 'subsample': 0.9232009658684728}. Best is trial 6 with value: 2.9920208906812245.\u001b[0m\n","\u001b[32m[I 2023-03-01 11:09:01,195]\u001b[0m Trial 7 finished with value: 3.1020160641252064 and parameters: {'num_leaves': 15, 'colsample_bytree': 0.8238886539454376, 'reg_alpha': 0.18354969377389296, 'reg_lambda': 5.859902699897871, 'max_depth': 14, 'n_estimators': 2390, 'min_child_samples': 13, 'subsample': 0.9404492019752718}. Best is trial 6 with value: 2.9920208906812245.\u001b[0m\n","\u001b[32m[I 2023-03-01 11:13:21,531]\u001b[0m Trial 8 finished with value: 3.022301894784829 and parameters: {'num_leaves': 39, 'colsample_bytree': 0.8092770967572402, 'reg_alpha': 0.4836061905063387, 'reg_lambda': 5.152245913332608, 'max_depth': 15, 'n_estimators': 2285, 'min_child_samples': 40, 'subsample': 0.700405907822624}. Best is trial 6 with value: 2.9920208906812245.\u001b[0m\n","\u001b[32m[I 2023-03-01 11:16:47,517]\u001b[0m Trial 9 finished with value: 3.379364577882724 and parameters: {'num_leaves': 23, 'colsample_bytree': 0.7275030166306214, 'reg_alpha': 0.53485391876583, 'reg_lambda': 3.407642186475901, 'max_depth': 3, 'n_estimators': 1850, 'min_child_samples': 93, 'subsample': 0.5176378637035883}. Best is trial 6 with value: 2.9920208906812245.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Best Score: 2.9920208906812245\n","Best trial: {'num_leaves': 114, 'colsample_bytree': 0.7654675794588895, 'reg_alpha': 0.1871725370949876, 'reg_lambda': 7.297792387589718, 'max_depth': 14, 'n_estimators': 1236, 'min_child_samples': 15, 'subsample': 0.9232009658684728}\n"]}]},{"cell_type":"code","source":["sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","\n","\n","params =  {'objective' : 'regression', 'device' : 'gpu', 'metric' : 'mae', 'num_leaves': 114, 'colsample_bytree': 0.7654675794588895, \n","           'reg_alpha': 0.1871725370949876, 'reg_lambda': 7.297792387589718, 'max_depth': 14, 'n_estimators': 1236, 'min_child_samples': 15, \n","           'subsample': 0.9232009658684728, 'learning_rate' : 0.015\n","}\n","\n","\n","skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=2023)\n","\n","folds = []\n","\n","for train_idx, val_idx in skf.split(X, y):\n","    folds.append((train_idx, val_idx))\n","\n","lgbm_model = {}\n","\n","for f in range(9):\n","    print(\n","        f'===================================={f+1}============================================')\n","    train_idx, val_idx = folds[f]\n","\n","    x_train, x_val, y_train, y_val = X.iloc[train_idx], X.iloc[val_idx], y.iloc[train_idx], y.iloc[val_idx]\n","\n","    lgbm = LGBMRegressor(**params)\n","    lgbm.fit(x_train, y_train)\n","\n","    y_pred = lgbm.predict(x_val)\n","    mae = mean_absolute_error(y_val, y_pred)\n","    print(f\"{f + 1} Fold MAE = {mae}\")\n","    lgbm_model[f] = lgbm\n","    print(f'================================================================================\\n\\n')\n","\n","\n","\n","for fold in range(9):\n","    sample_submission['target'] += lgbm_model[fold].predict(test)/9\n","\n","sample_submission.to_csv(PATH+\"/sample_submission_7.csv\", index = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vD03yWl5pmB","executionInfo":{"status":"ok","timestamp":1677672171826,"user_tz":-540,"elapsed":2429026,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"c59ab260-3bc9-4585-a3ea-c97d6e7b5efd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================1============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2141\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.092933 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788459\n","1 Fold MAE = 3.1608015203876887\n","================================================================================\n","\n","\n","====================================2============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2140\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.139793 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788444\n","2 Fold MAE = 3.156547734667791\n","================================================================================\n","\n","\n","====================================3============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2138\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.109040 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788385\n","3 Fold MAE = 3.1557067387815874\n","================================================================================\n","\n","\n","====================================4============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2142\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.091227 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788384\n","4 Fold MAE = 3.1575153252907806\n","================================================================================\n","\n","\n","====================================5============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2140\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.101717 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788458\n","5 Fold MAE = 3.151456568180448\n","================================================================================\n","\n","\n","====================================6============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2143\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.097410 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788449\n","6 Fold MAE = 3.157565595062086\n","================================================================================\n","\n","\n","====================================7============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2141\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.089314 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788438\n","7 Fold MAE = 3.155878739418378\n","================================================================================\n","\n","\n","====================================8============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2141\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.088157 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788478\n","8 Fold MAE = 3.1666747233619232\n","================================================================================\n","\n","\n","====================================9============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2141\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 25\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 21 dense feature groups (95.65 MB) transferred to GPU in 0.091553 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788481\n","9 Fold MAE = 3.155469885457928\n","================================================================================\n","\n","\n"]}]},{"cell_type":"code","source":["sub4 = pd.read_csv(PATH+\"/sample_submission_7.csv\")"],"metadata":{"id":"RmtjlOu6BCLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub4['target'] = np.round(sub4['target'])"],"metadata":{"id":"cOzXEDjvDhaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","sample_submission['target'] = sub4['target']\n","sample_submission.to_csv(PATH+\"/sample_submission_fin4.csv\", index = False)"],"metadata":{"id":"t5cglA5iELjf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# XGBoost2"],"metadata":{"id":"BPgL__NCl4_M"}},{"cell_type":"code","source":["train = train.drop(['base_date', 'date', 'year_month', 'road_type', 'road_rating', 'road_hour', 'road_day', 'start_turn_restricted', 'end_turn_restricted', 'road_name'],axis=1)\n","test = test.drop(['base_date', 'date', 'year_month', 'road_type', 'road_rating', 'road_hour', 'road_day', 'start_turn_restricted', 'end_turn_restricted', 'road_name'],axis=1)"],"metadata":{"id":"oiE9SLMymE0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train.drop(['target'], axis = 1)\n","y = train.target\n","target = test[X.columns]"],"metadata":{"id":"7o1mcXP5mlhP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","xgb_param1 = {\n","    'objective' : 'reg:absoluteerror',\n","    'tree_method' : 'gpu_hist',\n","    'predictor' : 'gpu_predictor'\n","}\n","\n","\n","xgb_pred = np.zeros(target.shape[0])\n","i = 0\n","xgb_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    xgb = XGBRegressor(**xgb_param1)\n","    xgb.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], early_stopping_rounds = 8, verbose = 1000, eval_metric = 'mae')\n","\n","    val_pred = xgb.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    xgb_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = xgb.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677664775034,"user_tz":-540,"elapsed":140953,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"e603ff2b-d417-4568-e455-f25a99c1f62b","id":"JscMl4XgmrMq"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-mae:4.29809\tvalidation_1-mae:4.30126\n","[99]\tvalidation_0-mae:3.25783\tvalidation_1-mae:3.27240\n","1 Fold MAE = 3.273750218028512\n","[0]\tvalidation_0-mae:4.29495\tvalidation_1-mae:4.29473\n","[99]\tvalidation_0-mae:3.20954\tvalidation_1-mae:3.21931\n","2 Fold MAE = 3.2203810925674614\n","[0]\tvalidation_0-mae:4.29510\tvalidation_1-mae:4.29426\n","[82]\tvalidation_0-mae:3.26586\tvalidation_1-mae:3.27240\n","3 Fold MAE = 3.2722378446445815\n","[0]\tvalidation_0-mae:4.29525\tvalidation_1-mae:4.29039\n","[99]\tvalidation_0-mae:3.26033\tvalidation_1-mae:3.26499\n","4 Fold MAE = 3.2725654191890614\n","[0]\tvalidation_0-mae:4.29985\tvalidation_1-mae:4.29778\n","[99]\tvalidation_0-mae:3.22564\tvalidation_1-mae:3.23514\n","5 Fold MAE = 3.2506923734690143\n","[0]\tvalidation_0-mae:4.29589\tvalidation_1-mae:4.30170\n","[99]\tvalidation_0-mae:3.18488\tvalidation_1-mae:3.20379\n","6 Fold MAE = 3.226307213872144\n","[0]\tvalidation_0-mae:4.32806\tvalidation_1-mae:4.32366\n","[79]\tvalidation_0-mae:3.31191\tvalidation_1-mae:3.31908\n","7 Fold MAE = 3.3191022755795303\n","[0]\tvalidation_0-mae:4.29514\tvalidation_1-mae:4.29567\n","[91]\tvalidation_0-mae:3.28384\tvalidation_1-mae:3.29557\n","8 Fold MAE = 3.2955536978777804\n","[0]\tvalidation_0-mae:4.30040\tvalidation_1-mae:4.30528\n","[99]\tvalidation_0-mae:3.25027\tvalidation_1-mae:3.26839\n","9 Fold MAE = 3.2686712569742684\n","[0]\tvalidation_0-mae:4.30020\tvalidation_1-mae:4.30000\n","[99]\tvalidation_0-mae:3.23870\tvalidation_1-mae:3.24437\n","10 Fold MAE = 3.2440478089683293\n","\n","AVG of MAE = 3.264330920117068\n"]}]},{"cell_type":"code","source":["import optuna\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def objective(trial,data=X,target=y):\n","    \n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2 ,random_state=2023)\n","    param = {\n","        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n","        'gamma': trial.suggest_uniform('gamma', 0.0, 5.0),\n","        'lambda': trial.suggest_uniform('lambda', 0.0, 5.0),\n","        'alpha': trial.suggest_uniform('alpha', 1e-3, 1.0),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5,0.7,0.9,1.0]),\n","        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n","        'n_estimators': trial.suggest_categorical('n_estimators', [100,300,500,700,1000]),\n","        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11]),\n","        'random_state': trial.suggest_categorical('random_state', [2023]),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n","    }\n","\n","\n","    model = XGBRegressor(**param)  \n","    \n","    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)],early_stopping_rounds=50,verbose=False)\n","    \n","    preds = model.predict(X_valid)\n","    \n","    mae = mean_absolute_error(y_valid, preds)\n","    \n","    return mae"],"metadata":{"id":"nhEq_esbnRjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from optuna.samplers import TPESampler\n","\n","sampler = TPESampler(seed=2023)\n","study = optuna.create_study(\n","    study_name=\"xgb_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=10)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXXjwra-nVDO","executionInfo":{"status":"ok","timestamp":1677665173781,"user_tz":-540,"elapsed":355294,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"5d085f44-023b-460a-ec95-763bf56e428b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-03-01 10:00:17,088]\u001b[0m A new study created in memory with name: xgb_parameter_opt\u001b[0m\n","\u001b[32m[I 2023-03-01 10:00:45,547]\u001b[0m Trial 0 finished with value: 3.0354970969960324 and parameters: {'gamma': 1.6099415199598388, 'lambda': 4.452112258947404, 'alpha': 0.5884642031950233, 'colsample_bytree': 0.9, 'subsample': 0.4, 'n_estimators': 300, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 0 with value: 3.0354970969960324.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:01:01,094]\u001b[0m Trial 1 finished with value: 3.0572209161297192 and parameters: {'gamma': 1.8828189138133906, 'lambda': 0.920270709137832, 'alpha': 0.10484788451657534, 'colsample_bytree': 1.0, 'subsample': 1.0, 'n_estimators': 100, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 49}. Best is trial 0 with value: 3.0354970969960324.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:02:00,168]\u001b[0m Trial 2 finished with value: 2.9619445368295683 and parameters: {'gamma': 2.0565552716077926, 'lambda': 3.6083220164655994, 'alpha': 0.6636241929424768, 'colsample_bytree': 1.0, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 11, 'random_state': 2023, 'min_child_weight': 50}. Best is trial 2 with value: 2.9619445368295683.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:02:52,430]\u001b[0m Trial 3 finished with value: 3.0201506729091205 and parameters: {'gamma': 3.767017216870225, 'lambda': 1.8290995790131896, 'alpha': 0.611761708308379, 'colsample_bytree': 0.9, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 32}. Best is trial 2 with value: 2.9619445368295683.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:03:23,399]\u001b[0m Trial 4 finished with value: 2.960761582884222 and parameters: {'gamma': 2.574423762029351, 'lambda': 4.474536589684463, 'alpha': 0.5191467602582522, 'colsample_bytree': 0.9, 'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:04:16,753]\u001b[0m Trial 5 finished with value: 3.00712654094277 and parameters: {'gamma': 4.668229769407715, 'lambda': 2.272017301864432, 'alpha': 0.494770923619669, 'colsample_bytree': 0.9, 'subsample': 0.7, 'n_estimators': 1000, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 11}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:04:57,073]\u001b[0m Trial 6 finished with value: 3.068972122357784 and parameters: {'gamma': 4.295779042178738, 'lambda': 3.303783585079416, 'alpha': 0.8729140014403514, 'colsample_bytree': 0.5, 'subsample': 0.5, 'n_estimators': 700, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 36}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:05:15,654]\u001b[0m Trial 7 finished with value: 3.212045327072068 and parameters: {'gamma': 3.8793464429587705, 'lambda': 3.4202519850916735, 'alpha': 0.7980996949121386, 'colsample_bytree': 0.7, 'subsample': 0.7, 'n_estimators': 300, 'max_depth': 5, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:05:56,144]\u001b[0m Trial 8 finished with value: 2.9662970864414397 and parameters: {'gamma': 4.628070935506319, 'lambda': 4.866981446349748, 'alpha': 0.5599798828202786, 'colsample_bytree': 0.9, 'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 30}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n","\u001b[32m[I 2023-03-01 10:06:12,256]\u001b[0m Trial 9 finished with value: 3.056623450866198 and parameters: {'gamma': 2.436534218353119, 'lambda': 0.5183649147923891, 'alpha': 0.29493011200957847, 'colsample_bytree': 1.0, 'subsample': 0.8, 'n_estimators': 100, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 30}. Best is trial 4 with value: 2.960761582884222.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Best Score: 2.960761582884222\n","Best trial: {'gamma': 2.574423762029351, 'lambda': 4.474536589684463, 'alpha': 0.5191467602582522, 'colsample_bytree': 0.9, 'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 17}\n"]}]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","xgb_param1 = {\n","    'objective' : 'reg:absoluteerror',\n","    'tree_method' : 'gpu_hist',\n","    'predictor' : 'gpu_predictor',\n","    'gamma': 2.774423762029351, 'lambda': 4.474536589684463, 'alpha': 0.5191467602582522, \n","    'colsample_bytree': 0.9, 'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 8, \n","    'random_state': 2023, 'min_child_weight': 17\n","}\n","\n","\n","xgb_pred = np.zeros(target.shape[0])\n","i = 0\n","xgb_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    xgb = XGBRegressor(**xgb_param1)\n","    xgb.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], early_stopping_rounds = 8, verbose = 1000, eval_metric = 'mae')\n","\n","    val_pred = xgb.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    xgb_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = xgb.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Y-mr5ttnd_7","executionInfo":{"status":"ok","timestamp":1677666066341,"user_tz":-540,"elapsed":185619,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"1a6347b0-4de4-4557-da70-20e356972aba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-mae:4.22064\tvalidation_1-mae:4.22653\n","[153]\tvalidation_0-mae:3.05924\tvalidation_1-mae:3.13286\n","1 Fold MAE = 3.1577441600265463\n","[0]\tvalidation_0-mae:4.21466\tvalidation_1-mae:4.21533\n","[165]\tvalidation_0-mae:3.02530\tvalidation_1-mae:3.10095\n","2 Fold MAE = 3.1264969518550503\n","[0]\tvalidation_0-mae:4.25605\tvalidation_1-mae:4.25642\n","[135]\tvalidation_0-mae:3.04546\tvalidation_1-mae:3.10883\n","3 Fold MAE = 3.132531555638749\n","[0]\tvalidation_0-mae:4.26106\tvalidation_1-mae:4.25657\n","[134]\tvalidation_0-mae:3.04346\tvalidation_1-mae:3.10102\n","4 Fold MAE = 3.1263544356571273\n","[0]\tvalidation_0-mae:4.25893\tvalidation_1-mae:4.25825\n","[160]\tvalidation_0-mae:3.03102\tvalidation_1-mae:3.09802\n","5 Fold MAE = 3.12178328178643\n","[0]\tvalidation_0-mae:4.25486\tvalidation_1-mae:4.26159\n","[139]\tvalidation_0-mae:3.03044\tvalidation_1-mae:3.09979\n","6 Fold MAE = 3.124229455332871\n","[0]\tvalidation_0-mae:4.25207\tvalidation_1-mae:4.24976\n","[149]\tvalidation_0-mae:3.04872\tvalidation_1-mae:3.11170\n","7 Fold MAE = 3.1345948498474865\n","[0]\tvalidation_0-mae:4.25562\tvalidation_1-mae:4.25673\n","[158]\tvalidation_0-mae:3.03710\tvalidation_1-mae:3.10355\n","8 Fold MAE = 3.125648503257672\n","[0]\tvalidation_0-mae:4.25164\tvalidation_1-mae:4.25458\n","[160]\tvalidation_0-mae:3.00942\tvalidation_1-mae:3.09011\n","9 Fold MAE = 3.1139749128415875\n","[0]\tvalidation_0-mae:4.25426\tvalidation_1-mae:4.25369\n","[151]\tvalidation_0-mae:3.02364\tvalidation_1-mae:3.08259\n","10 Fold MAE = 3.1068171811086933\n","\n","AVG of MAE = 3.1270175287352213\n"]}]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","\n","sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","\n","\n","params = {'objective' : 'reg:absoluteerror','tree_method' : 'gpu_hist', \n","          'predictor' : 'gpu_predictor', \n","          'gamma': 2.774423762029351, 'lambda': 4.474536589684463, 'alpha': 0.5191467602582522, 'colsample_bytree': 0.9, \n","          'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 8, 'random_state': 2023, 'min_child_weight': 17, 'eta' : 0.01\n","          }\n","\n","\n","skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=404)\n","\n","folds = []\n","\n","for train_idx, val_idx in skf.split(X, y):\n","    folds.append((train_idx, val_idx))\n","\n","XGB_model = {}\n","\n","for f in range(9):\n","    print(\n","        f'===================================={f+1}============================================')\n","    train_idx, val_idx = folds[f]\n","\n","    x_train, x_val, y_train, y_val = X.iloc[train_idx], X.iloc[val_idx], y.iloc[train_idx], y.iloc[val_idx]\n","\n","    XGB = XGBRegressor(**params)\n","    XGB.fit(x_train, y_train)\n","\n","    y_pred = XGB.predict(x_val)\n","    mae = mean_absolute_error(y_val, y_pred)\n","    print(f\"{f + 1} Fold MAE = {mae}\")\n","    XGB_model[f] = XGB\n","    print(f'================================================================================\\n\\n')\n","\n","\n","\n","\n","for fold in range(9):\n","    sample_submission['target'] += XGB_model[fold].predict(test)/9\n","\n","sample_submission.to_csv(PATH+\"/sample_submission_6.csv\", index = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdqeFFHKsban","executionInfo":{"status":"ok","timestamp":1677666764791,"user_tz":-540,"elapsed":391739,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"1b464bef-9ee6-4a1e-f9a3-f8fc030fbffe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================1============================================\n","1 Fold MAE = 3.1128873129295096\n","================================================================================\n","\n","\n","====================================2============================================\n","2 Fold MAE = 3.091132997036504\n","================================================================================\n","\n","\n","====================================3============================================\n","3 Fold MAE = 3.0814317551326074\n","================================================================================\n","\n","\n","====================================4============================================\n","4 Fold MAE = 3.085421789258399\n","================================================================================\n","\n","\n","====================================5============================================\n","5 Fold MAE = 3.0904089965686525\n","================================================================================\n","\n","\n","====================================6============================================\n","6 Fold MAE = 3.1249602144274147\n","================================================================================\n","\n","\n","====================================7============================================\n","7 Fold MAE = 3.087142298759932\n","================================================================================\n","\n","\n","====================================8============================================\n","8 Fold MAE = 3.1130516248685582\n","================================================================================\n","\n","\n","====================================9============================================\n","9 Fold MAE = 3.112618992178237\n","================================================================================\n","\n","\n"]}]},{"cell_type":"code","source":["sub3 = pd.read_csv(PATH+\"/sample_submission_6.csv\")"],"metadata":{"id":"mLBt006Lu-XB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"7oZZEX6CvEoU","executionInfo":{"status":"ok","timestamp":1677666834008,"user_tz":-540,"elapsed":5,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"5123dca1-9db6-49bb-b107-f99e689508e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id     target\n","0       TEST_000000  26.181024\n","1       TEST_000001  42.041897\n","2       TEST_000002  64.818620\n","3       TEST_000003  36.542410\n","4       TEST_000004  48.509133\n","...             ...        ...\n","291236  TEST_291236  48.380587\n","291237  TEST_291237  51.440572\n","291238  TEST_291238  22.940762\n","291239  TEST_291239  31.602361\n","291240  TEST_291240  52.749055\n","\n","[291241 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-48a0c55f-190b-43ea-a27e-339930b01db7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_000000</td>\n","      <td>26.181024</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_000001</td>\n","      <td>42.041897</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_000002</td>\n","      <td>64.818620</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_000003</td>\n","      <td>36.542410</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_000004</td>\n","      <td>48.509133</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>291236</th>\n","      <td>TEST_291236</td>\n","      <td>48.380587</td>\n","    </tr>\n","    <tr>\n","      <th>291237</th>\n","      <td>TEST_291237</td>\n","      <td>51.440572</td>\n","    </tr>\n","    <tr>\n","      <th>291238</th>\n","      <td>TEST_291238</td>\n","      <td>22.940762</td>\n","    </tr>\n","    <tr>\n","      <th>291239</th>\n","      <td>TEST_291239</td>\n","      <td>31.602361</td>\n","    </tr>\n","    <tr>\n","      <th>291240</th>\n","      <td>TEST_291240</td>\n","      <td>52.749055</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>291241 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48a0c55f-190b-43ea-a27e-339930b01db7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-48a0c55f-190b-43ea-a27e-339930b01db7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-48a0c55f-190b-43ea-a27e-339930b01db7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","sample_submission['target'] = np.round((sub1['target'] + sub2['target'] + sub3['target']) / 3)\n","sample_submission.to_csv(PATH+\"/sample_submission_fin2.csv\", index = False)"],"metadata":{"id":"0ftLcUzGuzbm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# XGBoost 1"],"metadata":{"id":"TSnbKGQIkZKo"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from catboost import CatBoostRegressor, Pool\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor"],"metadata":{"id":"sxiZ28pMRMhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train.drop(['base_date', 'date', 'year_month', 'road_type', 'road_rating', 'road_hour', 'road_day', 'start_turn_restricted', 'end_turn_restricted'],axis=1)\n","test = test.drop(['base_date', 'date', 'year_month', 'road_type', 'road_rating', 'road_hour', 'road_day', 'start_turn_restricted', 'end_turn_restricted'],axis=1)"],"metadata":{"id":"nD6iDnVbkdvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train.drop(['target'], axis = 1)\n","y = train.target\n","target = test[X.columns]"],"metadata":{"id":"_O84eLlFmPMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","xgb_param1 = {\n","    'objective' : 'reg:absoluteerror',\n","    'tree_method' : 'gpu_hist',\n","    'predictor' : 'gpu_predictor'\n","}\n","\n","\n","xgb_pred = np.zeros(target.shape[0])\n","i = 0\n","xgb_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    xgb = XGBRegressor(**xgb_param1)\n","    xgb.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], early_stopping_rounds = 8, verbose = 1000, eval_metric = 'mae')\n","\n","    val_pred = xgb.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    xgb_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = xgb.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LS18jbAlnpwY","executionInfo":{"status":"ok","timestamp":1677649464310,"user_tz":-540,"elapsed":147904,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"0f8fa092-b649-4b48-d7b3-1d0aae298885"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-mae:4.29809\tvalidation_1-mae:4.30126\n","[48]\tvalidation_0-mae:3.33140\tvalidation_1-mae:3.34027\n","1 Fold MAE = 3.3402478505579403\n","[0]\tvalidation_0-mae:4.29495\tvalidation_1-mae:4.29473\n","[83]\tvalidation_0-mae:3.29789\tvalidation_1-mae:3.30278\n","2 Fold MAE = 3.302629955628539\n","[0]\tvalidation_0-mae:4.29519\tvalidation_1-mae:4.29439\n","[63]\tvalidation_0-mae:3.34215\tvalidation_1-mae:3.34606\n","3 Fold MAE = 3.345982532193771\n","[0]\tvalidation_0-mae:4.29522\tvalidation_1-mae:4.29047\n","[99]\tvalidation_0-mae:3.21398\tvalidation_1-mae:3.21800\n","4 Fold MAE = 3.2184986024904174\n","[0]\tvalidation_0-mae:4.29985\tvalidation_1-mae:4.29778\n","[99]\tvalidation_0-mae:3.23984\tvalidation_1-mae:3.24667\n","5 Fold MAE = 3.246882724058861\n","[0]\tvalidation_0-mae:4.29589\tvalidation_1-mae:4.30170\n","[99]\tvalidation_0-mae:3.23126\tvalidation_1-mae:3.24979\n","6 Fold MAE = 3.2678219696163975\n","[0]\tvalidation_0-mae:4.32803\tvalidation_1-mae:4.32370\n","[99]\tvalidation_0-mae:3.23003\tvalidation_1-mae:3.24134\n","7 Fold MAE = 3.2413947868851065\n","[0]\tvalidation_0-mae:4.29514\tvalidation_1-mae:4.29567\n","[95]\tvalidation_0-mae:3.27120\tvalidation_1-mae:3.28142\n","8 Fold MAE = 3.281455199831533\n","[0]\tvalidation_0-mae:4.30040\tvalidation_1-mae:4.30528\n","[99]\tvalidation_0-mae:3.24357\tvalidation_1-mae:3.25805\n","9 Fold MAE = 3.2581697052460963\n","[0]\tvalidation_0-mae:4.30020\tvalidation_1-mae:4.30000\n","[99]\tvalidation_0-mae:3.26822\tvalidation_1-mae:3.27380\n","10 Fold MAE = 3.2745825011007805\n","\n","AVG of MAE = 3.2777665827609446\n"]}]},{"cell_type":"code","source":["import optuna\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def objective(trial,data=X,target=y):\n","    \n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2 ,random_state=2023)\n","    param = {\n","        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n","        'gamma': trial.suggest_uniform('gamma', 0.0, 5.0),\n","        'lambda': trial.suggest_uniform('lambda', 0.0, 5.0),\n","        'alpha': trial.suggest_uniform('alpha', 1e-3, 1.0),\n","        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5,0.7,0.9,1.0]),\n","        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n","        'n_estimators': trial.suggest_categorical('n_estimators', [100,300,500,700,1000]),\n","        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11]),\n","        'random_state': trial.suggest_categorical('random_state', [2023]),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n","    }\n","\n","\n","    model = XGBRegressor(**param)  \n","    \n","    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)],early_stopping_rounds=50,verbose=False)\n","    \n","    preds = model.predict(X_valid)\n","    \n","    mae = mean_absolute_error(y_valid, preds)\n","    \n","    return mae"],"metadata":{"id":"hyhy61PU9GCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from optuna.samplers import TPESampler\n","\n","sampler = TPESampler(seed=2023)\n","study = optuna.create_study(\n","    study_name=\"xgb_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=10)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAUF9G3Z9HTk","executionInfo":{"status":"ok","timestamp":1677654329435,"user_tz":-540,"elapsed":390153,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"5b5f31e0-4a2f-4de4-d1df-6fdb23a46960"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-03-01 06:58:57,887]\u001b[0m A new study created in memory with name: xgb_parameter_opt\u001b[0m\n","\u001b[32m[I 2023-03-01 06:59:28,924]\u001b[0m Trial 0 finished with value: 3.032706913212054 and parameters: {'gamma': 1.6099415199598388, 'lambda': 4.452112258947404, 'alpha': 0.5884642031950233, 'colsample_bytree': 0.9, 'subsample': 0.4, 'n_estimators': 300, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 0 with value: 3.032706913212054.\u001b[0m\n","\u001b[32m[I 2023-03-01 06:59:45,631]\u001b[0m Trial 1 finished with value: 3.052021485796651 and parameters: {'gamma': 1.8828189138133906, 'lambda': 0.920270709137832, 'alpha': 0.10484788451657534, 'colsample_bytree': 1.0, 'subsample': 1.0, 'n_estimators': 100, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 49}. Best is trial 0 with value: 3.032706913212054.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:00:54,619]\u001b[0m Trial 2 finished with value: 2.9570779279179633 and parameters: {'gamma': 2.0565552716077926, 'lambda': 3.6083220164655994, 'alpha': 0.6636241929424768, 'colsample_bytree': 1.0, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 11, 'random_state': 2023, 'min_child_weight': 50}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:01:50,998]\u001b[0m Trial 3 finished with value: 3.0197583244601938 and parameters: {'gamma': 3.767017216870225, 'lambda': 1.8290995790131896, 'alpha': 0.611761708308379, 'colsample_bytree': 0.9, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 32}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:02:25,393]\u001b[0m Trial 4 finished with value: 2.958170390604129 and parameters: {'gamma': 2.574423762029351, 'lambda': 4.474536589684463, 'alpha': 0.5191467602582522, 'colsample_bytree': 0.9, 'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:03:21,661]\u001b[0m Trial 5 finished with value: 3.002768762315707 and parameters: {'gamma': 4.668229769407715, 'lambda': 2.272017301864432, 'alpha': 0.494770923619669, 'colsample_bytree': 0.9, 'subsample': 0.7, 'n_estimators': 1000, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 11}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:04:03,975]\u001b[0m Trial 6 finished with value: 3.07216470738779 and parameters: {'gamma': 4.295779042178738, 'lambda': 3.303783585079416, 'alpha': 0.8729140014403514, 'colsample_bytree': 0.5, 'subsample': 0.5, 'n_estimators': 700, 'max_depth': 7, 'random_state': 2023, 'min_child_weight': 36}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:04:27,326]\u001b[0m Trial 7 finished with value: 3.2119737834587916 and parameters: {'gamma': 3.8793464429587705, 'lambda': 3.4202519850916735, 'alpha': 0.7980996949121386, 'colsample_bytree': 0.7, 'subsample': 0.7, 'n_estimators': 300, 'max_depth': 5, 'random_state': 2023, 'min_child_weight': 17}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:05:09,723]\u001b[0m Trial 8 finished with value: 2.9631888168625156 and parameters: {'gamma': 4.628070935506319, 'lambda': 4.866981446349748, 'alpha': 0.5599798828202786, 'colsample_bytree': 0.9, 'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 30}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n","\u001b[32m[I 2023-03-01 07:05:27,124]\u001b[0m Trial 9 finished with value: 3.0619646624787986 and parameters: {'gamma': 2.436534218353119, 'lambda': 0.5183649147923891, 'alpha': 0.29493011200957847, 'colsample_bytree': 1.0, 'subsample': 0.8, 'n_estimators': 100, 'max_depth': 9, 'random_state': 2023, 'min_child_weight': 30}. Best is trial 2 with value: 2.9570779279179633.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Best Score: 2.9570779279179633\n","Best trial: {'gamma': 2.0565552716077926, 'lambda': 3.6083220164655994, 'alpha': 0.6636241929424768, 'colsample_bytree': 1.0, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 11, 'random_state': 2023, 'min_child_weight': 50}\n"]}]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","xgb_param1 = {\n","    'objective' : 'reg:absoluteerror','tree_method' : 'gpu_hist', \n","    'predictor' : 'gpu_predictor', 'gamma': 2.0565552716077926, 'lambda': 3.6083220164655994, 'alpha': 0.6636241929424768, \n","    'colsample_bytree': 1.0, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 8, 'random_state': 2023, 'min_child_weight': 50, 'eta' : 0.015\n","}\n","\n","\n","xgb_pred = np.zeros(target.shape[0])\n","i = 0\n","xgb_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    xgb = XGBRegressor(**xgb_param1)\n","    xgb.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], early_stopping_rounds = 8, verbose = 1000, eval_metric = 'mae')\n","\n","    val_pred = xgb.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    xgb_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = xgb.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"id":"ifBVbfCvAAyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","\n","sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","\n","\n","params = {'objective' : 'reg:absoluteerror','tree_method' : 'gpu_hist', \n","          'predictor' : 'gpu_predictor', 'gamma': 2.0565552716077926, 'lambda': 3.6083220164655994, 'alpha': 0.6636241929424768, \n","          'colsample_bytree': 1.0, 'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 8, 'random_state': 2023, 'min_child_weight': 50, 'eta' : 0.015\n","          }\n","\n","\n","skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=404)\n","\n","folds = []\n","\n","for train_idx, val_idx in skf.split(X, y):\n","    folds.append((train_idx, val_idx))\n","\n","XGB_model = {}\n","\n","for f in range(9):\n","    print(\n","        f'===================================={f+1}============================================')\n","    train_idx, val_idx = folds[f]\n","\n","    x_train, x_val, y_train, y_val = X.iloc[train_idx], X.iloc[val_idx], y.iloc[train_idx], y.iloc[val_idx]\n","\n","    XGB = XGBRegressor(**params)\n","    XGB.fit(x_train, y_train)\n","\n","    y_pred = XGB.predict(x_val)\n","    mae = mean_absolute_error(y_val, y_pred)\n","    print(f\"{f + 1} Fold MAE = {mae}\")\n","    XGB_model[f] = XGB\n","    print(f'================================================================================\\n\\n')\n","\n","\n","\n","\n","for fold in range(9):\n","    sample_submission['target'] += XGB_model[fold].predict(test)/9\n","\n","sample_submission.to_csv(PATH+\"/sample_submission_4.csv\", index = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2KzDBoBQzVE","executionInfo":{"status":"ok","timestamp":1677660031876,"user_tz":-540,"elapsed":645142,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"4e941237-943b-4154-e46c-7c4b8fd2da6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================1============================================\n","1 Fold MAE = 3.0772075207821974\n","================================================================================\n","\n","\n","====================================2============================================\n","2 Fold MAE = 3.0763008442294355\n","================================================================================\n","\n","\n","====================================3============================================\n","3 Fold MAE = 3.0688078435139845\n","================================================================================\n","\n","\n","====================================4============================================\n","4 Fold MAE = 3.0693848314425822\n","================================================================================\n","\n","\n","====================================5============================================\n","5 Fold MAE = 3.0715137126931378\n","================================================================================\n","\n","\n","====================================6============================================\n","6 Fold MAE = 3.0769248957925694\n","================================================================================\n","\n","\n","====================================7============================================\n","7 Fold MAE = 3.0805428913244626\n","================================================================================\n","\n","\n","====================================8============================================\n","8 Fold MAE = 3.071684182762709\n","================================================================================\n","\n","\n","====================================9============================================\n","9 Fold MAE = 3.0727304044988126\n","================================================================================\n","\n","\n"]}]},{"cell_type":"code","source":["sub1 = pd.read_csv(PATH+\"/sample_submission_4.csv\")"],"metadata":{"id":"htFklUJFP2yS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"DCr6fCsYVWAf","executionInfo":{"status":"ok","timestamp":1677660089220,"user_tz":-540,"elapsed":4,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"e2a68454-0ef4-48c2-ebbc-90ac6dd5139b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id     target\n","0       TEST_000000  26.288453\n","1       TEST_000001  41.951781\n","2       TEST_000002  61.402993\n","3       TEST_000003  37.561128\n","4       TEST_000004  50.149511\n","...             ...        ...\n","291236  TEST_291236  49.141225\n","291237  TEST_291237  49.930929\n","291238  TEST_291238  22.329930\n","291239  TEST_291239  31.655704\n","291240  TEST_291240  50.944145\n","\n","[291241 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-f7a363c7-6cfe-41c4-8736-159056f19aa3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_000000</td>\n","      <td>26.288453</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_000001</td>\n","      <td>41.951781</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_000002</td>\n","      <td>61.402993</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_000003</td>\n","      <td>37.561128</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_000004</td>\n","      <td>50.149511</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>291236</th>\n","      <td>TEST_291236</td>\n","      <td>49.141225</td>\n","    </tr>\n","    <tr>\n","      <th>291237</th>\n","      <td>TEST_291237</td>\n","      <td>49.930929</td>\n","    </tr>\n","    <tr>\n","      <th>291238</th>\n","      <td>TEST_291238</td>\n","      <td>22.329930</td>\n","    </tr>\n","    <tr>\n","      <th>291239</th>\n","      <td>TEST_291239</td>\n","      <td>31.655704</td>\n","    </tr>\n","    <tr>\n","      <th>291240</th>\n","      <td>TEST_291240</td>\n","      <td>50.944145</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>291241 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7a363c7-6cfe-41c4-8736-159056f19aa3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f7a363c7-6cfe-41c4-8736-159056f19aa3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f7a363c7-6cfe-41c4-8736-159056f19aa3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# LGBM1"],"metadata":{"id":"u6sVm_naq53_"}},{"cell_type":"code","source":["from lightgbm import LGBMRegressor\n","\n","skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","lgbm_param1 = {\n","    'objective' : 'regression',\n","    'device' : 'gpu',\n","    'metric' : 'mae'\n","}\n","\n","\n","lgbm_pred1 = np.zeros(target.shape[0])\n","i = 0\n","lgbm_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    lgbm = LGBMRegressor(**lgbm_param1)\n","    lgbm.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], eval_metric = 'mae')\n","\n","    val_pred = lgbm.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    lgbm_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = lgbm.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITGT4mMxq5iB","executionInfo":{"status":"ok","timestamp":1677650571596,"user_tz":-540,"elapsed":242046,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"dd5754fa-d714-4d50-e706-2cf7f729200d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2204\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.108513 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788469\n","1 Fold MAE = 3.4196846775943266\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.099137 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788468\n","2 Fold MAE = 3.4208779848635036\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.165134 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788436\n","3 Fold MAE = 3.41852965825892\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.092897 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788426\n","4 Fold MAE = 3.411386831503312\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2202\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.099049 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788385\n","5 Fold MAE = 3.4135160660424315\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2204\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.094296 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788412\n","6 Fold MAE = 3.4205291392447066\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2200\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.092967 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788430\n","7 Fold MAE = 3.4149965328148864\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.103480 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788456\n","8 Fold MAE = 3.4191623007693766\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.103244 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788478\n","9 Fold MAE = 3.4211660402321957\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4231096, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.105758 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788460\n","10 Fold MAE = 3.410139091850821\n","\n","AVG of MAE = 3.2777665827609446\n"]}]},{"cell_type":"code","source":["import optuna\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def objective(trial,data=X,target=y):\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2 ,random_state=2023)\n","    param = {\n","        'device' : 'gpu',\n","        'objective': 'regression',\n","        'verbose': -1,\n","        'metric': 'mae', \n","        'num_leaves': trial.suggest_int('num_leaves', 2, 1024, step=1, log=True), \n","        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n","        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n","        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n","        'max_depth': trial.suggest_int('max_depth',3, 15),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n","        'subsample': trial.suggest_loguniform('subsample', 0.4, 1)\n","    }\n","\n","    # Generate model\n","    model = LGBMRegressor(**param)\n","    model.fit(X_train,y_train,eval_set=[(X_valid,y_valid)], eval_metric = 'mae')\n","    \n","    preds = model.predict(X_valid)\n","    \n","    mae = mean_absolute_error(y_valid, preds)\n","    \n","    return mae"],"metadata":{"id":"1baeLq9kw5S3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from optuna.samplers import TPESampler\n","\n","sampler = TPESampler(seed=2023)\n","study = optuna.create_study(\n","    study_name=\"lgbm_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=10)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"metadata":{"id":"miuI6OE3yBYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightgbm import LGBMRegressor\n","\n","skf = StratifiedKFold(n_splits = 10, random_state = 2023, shuffle = True)\n","\n","lgbm_param1 = {\n","    'objective' : 'regression',\n","    'device' : 'gpu',\n","    'metric' : 'mae',\n","    'num_leaves': 114, 'colsample_bytree': 0.7654675794588895, 'reg_alpha': 0.1871725370949876, 'reg_lambda': 7.297792387589718, 'max_depth': 14, 'n_estimators': 1236, 'min_child_samples': 15, 'subsample': 0.9232009658684728\n","}\n","\n","\n","lgbm_pred1 = np.zeros(target.shape[0])\n","i = 0\n","lgbm_mae = []\n","\n","for tr_idx, val_idx in skf.split(X, y):\n","    \n","    tr_x, tr_y = X.iloc[tr_idx], y.iloc[tr_idx]\n","    val_x, val_y = X.iloc[val_idx], y.iloc[val_idx]\n","\n","    lgbm = LGBMRegressor(**lgbm_param1)\n","    lgbm.fit(tr_x, tr_y, eval_set = [(tr_x, tr_y), (val_x, val_y)], eval_metric = 'mae')\n","\n","    val_pred = lgbm.predict(val_x).astype(int)\n","    fold_mae = mean_absolute_error(val_y, val_pred)\n","    lgbm_mae.append(fold_mae)\n","    print(f\"{i + 1} Fold MAE = {fold_mae}\")\n","\n","    i += 1\n","\n","    fold_pred = lgbm.predict(target) / skf.n_splits\n","    xgb_pred += fold_pred\n","\n","print(f\"\\nAVG of MAE = {np.mean(xgb_mae)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AFA1x6pI6E-A","executionInfo":{"status":"error","timestamp":1677653606441,"user_tz":-540,"elapsed":656120,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"84ae1da8-95d6-4456-b5ad-663b6a77ff21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2204\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.103469 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788469\n","1 Fold MAE = 3.0350504762593538\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.123492 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788468\n","2 Fold MAE = 3.0302815013975097\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.097171 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788436\n","3 Fold MAE = 3.0302261966042856\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4231095, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (96.84 MB) transferred to GPU in 0.151987 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788426\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-5d78fdcca804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlgbm_param1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     ) -> \"LGBMRegressor\":\n\u001b[1;32m   1008\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3491\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m                 ctypes.byref(is_finished)))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")\n","\n","\n","params =  {'objective' : 'regression', 'device' : 'gpu', 'metric' : 'mae', 'num_leaves': 114, 'colsample_bytree': 0.7654675794588895, \n","           'reg_alpha': 0.1871725370949876, 'reg_lambda': 7.297792387589718, 'max_depth': 14, 'n_estimators': 1236, 'min_child_samples': 15, \n","           'subsample': 0.9232009658684728, 'learning_rate' : 0.015\n","}\n","\n","\n","skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=404)\n","\n","folds = []\n","\n","for train_idx, val_idx in skf.split(X, y):\n","    folds.append((train_idx, val_idx))\n","\n","lgbm_model = {}\n","\n","for f in range(9):\n","    print(\n","        f'===================================={f+1}============================================')\n","    train_idx, val_idx = folds[f]\n","\n","    x_train, x_val, y_train, y_val = X.iloc[train_idx], X.iloc[val_idx], y.iloc[train_idx], y.iloc[val_idx]\n","\n","    lgbm = LGBMRegressor(**params)\n","    lgbm.fit(x_train, y_train)\n","\n","    y_pred = lgbm.predict(x_val)\n","    mae = mean_absolute_error(y_val, y_pred)\n","    print(f\"{f + 1} Fold MAE = {mae}\")\n","    lgbm_model[f] = lgbm\n","    print(f'================================================================================\\n\\n')\n","\n","\n","\n","for fold in range(9):\n","    sample_submission['target'] += lgbm_model[fold].predict(test)/9\n","\n","sample_submission.to_csv(PATH+\"/sample_submission_5.csv\", index = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5svfCWZf_jq0","executionInfo":{"status":"ok","timestamp":1677662613050,"user_tz":-540,"elapsed":2351120,"user":{"displayName":"목진휘","userId":"02250361518953256250"}},"outputId":"557fd0e4-bc97-4341-85bb-2e67b1d15460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================================1============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2204\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.100053 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788459\n","1 Fold MAE = 3.1594831283568756\n","================================================================================\n","\n","\n","====================================2============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2200\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.129277 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788444\n","2 Fold MAE = 3.154198136195423\n","================================================================================\n","\n","\n","====================================3============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.093801 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788385\n","3 Fold MAE = 3.144644444649795\n","================================================================================\n","\n","\n","====================================4============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2201\n","[LightGBM] [Info] Number of data points in the train set: 4178859, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.087796 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788384\n","4 Fold MAE = 3.152908260489334\n","================================================================================\n","\n","\n","====================================5============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2200\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.115931 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788458\n","5 Fold MAE = 3.1570384407028467\n","================================================================================\n","\n","\n","====================================6============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2202\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.137268 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788449\n","6 Fold MAE = 3.153658673268453\n","================================================================================\n","\n","\n","====================================7============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2202\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.091352 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788438\n","7 Fold MAE = 3.1546746835428308\n","================================================================================\n","\n","\n","====================================8============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2203\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.118446 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788478\n","8 Fold MAE = 3.157528803040988\n","================================================================================\n","\n","\n","====================================9============================================\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 2202\n","[LightGBM] [Info] Number of data points in the train set: 4178860, number of used features: 26\n","[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n","[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 8\n","[LightGBM] [Info] 22 dense feature groups (95.65 MB) transferred to GPU in 0.094826 secs. 1 sparse feature groups\n","[LightGBM] [Info] Start training from score 42.788481\n","9 Fold MAE = 3.156188705659623\n","================================================================================\n","\n","\n"]}]},{"cell_type":"code","source":["sub2 = pd.read_csv(PATH+\"/sample_submission_5.csv\")"],"metadata":{"id":"mxAJWUFWbrTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission = pd.read_csv(PATH+\"/sample_submission.csv\")"],"metadata":{"id":"sfzsjQ9Ebtxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission['target'] = np.round((sub1['target'] + sub2['target']) / 2)"],"metadata":{"id":"XGt0EVXdbx7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_submission.to_csv(PATH+\"/sample_submission_fin.csv\", index = False)"],"metadata":{"id":"CLFSDK_3b2y5"},"execution_count":null,"outputs":[]}]}